## Sentiment Analysis using a fine-tuned BERT model

### Full folder
I use Google Colab GPU to train my BERT model. Please find my folder with full files [here](https://drive.google.com/drive/folders/1SlHy_jb7erBASjifYhb3-SR9KweVaU8x?usp=sharing).

### Run
* [sentiment_analysis.ipynb](https://github.com/NaiyuJ/computational-text-analysis/blob/main/ethnicity_China/sentiment-analysis/sentiment_analysis.ipynb)

### Supporting files
* [stopwords.txt](https://github.com/NaiyuJ/computational-text-analysis/blob/main/ethnicity_China/sentiment-analysis/stopwords.txt)

### Input
* [total_cat.csv](https://drive.google.com/file/d/1gcYa-oD7z2HbaQeBYPanPAOiqpt8ovDW/view?usp=sharing)

### Output
* [total_sent.csv](https://drive.google.com/file/d/104G1CBD05Z7-mUw6gZ-1nbtXuSewLp1X/view?usp=sharing)

### Weibo corpus for fine-tuning (labeled)
* [weibo_senti_100k.csv](https://drive.google.com/file/d/10IlSR8RnsjrhTceIE3to9RHlE9WUqx1U/view?usp=sharing)

### Saved BERT model
* [beto-sentiment-analysis](https://drive.google.com/drive/folders/1-JGjmfq1AzHtgeOl1sna1NnCo51KpaPt?usp=sharing)
* [beto-sentiment-analysis_tokenizer](https://drive.google.com/drive/folders/1-Cohrdf9f6c6imtT-rQqoh6f-e0FF2Oz?usp=sharing)
